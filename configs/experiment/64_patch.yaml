# @package _global_

# to execute this experiment run:
# python src/clip_cache.py experiment=64_patch

defaults:
  - override /data: voc2012.yaml
  - override /model: default.yaml
  - override /paths: voc2012.yaml
  - override /hydra: default.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

# task name, determines output directory path (for logs) eg: [clip_cache, train, evaluate]
task_name: "clip_cache_64_patch"

# config for creating CLIP cache vectors
clip_cache:
  mode: "aggregate" # which cache to generate: [global, aggregate, all]
  batch_size: 32
  num_workers: 16
  thresh: 0.5
  snippet_size: 64
  global_cache_dir: ${paths.cache_save_root}/clip_cache/model_${model.clip.name}/global
  aggregate_cache_dir: ${paths.cache_save_root}/clip_cache/model_${model.clip.name}/thresh_${clip_cache.thresh}/snippet_size_${clip_cache.snippet_size}/aggregate
  pseudo_cache_dir: ${paths.cache_save_root}/clip_cache/model_${model.clip.name}/pseudo

evaluate: 
  mode: "aggregate_onehot" # mode of CLIP cache to evaluate
  batch_size: 8
  num_workers: 16

train:
  seed: 42
  target_transform: filename_global_onehot # 'filename_<pseudo label mode>_onehot'
  train_transform:
    _target_: src.data.data.ResNet101Transforms
  val_transform:
    _target_: src.data.data.ResNet101Transforms
  batch_size: 8
  num_workers: 16
  device: cuda
  optimizer:
    _target_: torch.optim.Adam
    lr: 1e-5
  loss:
    _target_: torchmetrics.regression.KLDivergence
  sigma: 1
  enable_checkpointing: false
  resume:
    ckpt_path: null
  max_epochs: 50