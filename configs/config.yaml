# @package _global_

# specify default configuration
# order of defaults determines the order in which configs override each other
defaults:
  - _self_
  - logger: wandb
  - paths: voc2012
  - data: voc2012
  - hydra: default
  - model: default

  # experiment configs allow for version control of specific hyperparameters
  # e.g. best hyperparameters for given model and datamodule
  - experiment: null

  # config for hyperparameter optimization
  # - hparams_search: null

# task name, determines output directory path (for logs) eg: [clip_cache, train, evaluate]
task_name: "clip_cache"

# config for creating CLIP cache vectors
clip_cache:
  mode: "global" # which cache to generate: [global, aggregate, all]
  batch_size: 8
  num_workers: 4
  thresh: 0.5
  snippet_size: 3
  global_cache_dir: ${paths.cache_save_root}/clip_cache/model_${model.clip.name}/global
  aggregate_cache_dir: ${paths.cache_save_root}/clip_cache/model_${model.clip.name}/thresh_${clip_cache.thresh}/snippet_size_${clip_cache.snippet_size}/aggregate
  pseudo_cache_dir: ${paths.cache_save_root}/clip_cache/model_${model.clip.name}/pseudo

evaluate: 
  mode: "global_onehot" # mode of CLIP cache to evaluate
  batch_size: 8
  num_workers: 4

train:
  seed: 42
  target_transform: filename_global_onehot # 'filename_<pseudo label mode: [global, aggregate, final]>_onehot'
  train_transform:
    _target_: src.data.data.ResNet101Transforms
  val_transform:
    _target_: src.data.data.ResNet101Transforms
  batch_size: 8
  num_workers: 16
  device: cuda
  optimizer:
    _target_: torch.optim.Adam
    lr: 1e-5
  loss:
    _target_: torchmetrics.regression.KLDivergence
  pseudo_update_frequency: 10 # frequency (in epochs) to update the psuedo labels
  sigma: 1
  enable_checkpointing: false
  resume:
    ckpt_path: null
  max_epochs: 100