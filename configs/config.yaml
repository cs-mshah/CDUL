# @package _global_

# specify default configuration
# order of defaults determines the order in which configs override each other
defaults:
  - _self_
  - logger: wandb
  - paths: voc2012
  - data: voc2012
  - hydra: default
  - model: default

  # experiment configs allow for version control of specific hyperparameters
  # e.g. best hyperparameters for given model and datamodule
  - experiment: null

  # config for hyperparameter optimization
  - hparams_search: null

# task name, determines output directory path (for logs) eg: [clip_cache, train, evaluate]
task_name: "voc2012_train"

# print verbose config
config_tree: true

# config for creating CLIP cache vectors
clip_cache:
  mode: "aggregate" # which cache to generate: [global, aggregate, all]
  batch_size: 16
  num_workers: 12
  thresh: 0 # supplementary (of original paper) range: [0, 0.4]
  # snippet_size: 3 # size of crops in pixels
  num_patches: 3 # snippet size (number of crops along a dimension of original image. 3 x 3 = 9 crops)
  gpus: 8
  global_cache_dir: ${paths.cache_save_root}/clip_cache/model_${model.clip.name}/global
  # alpha_beta_dir: ${paths.cache_save_root}/clip_cache/model_${model.clip.name}/snippet_size_${clip_cache.snippet_size}
  alpha_beta_dir: ${paths.cache_save_root}/clip_cache/model_${model.clip.name}/num_patches_${clip_cache.num_patches}
  aggregate_cache_dir: ${clip_cache.alpha_beta_dir}/thresh_${clip_cache.thresh}/aggregate
  pseudo_cache_dir: ${paths.cache_save_root}/clip_cache/model_${model.clip.name}/pseudo
  # lamda for computing final pseudo label. (see supplementary of original paper)
  final_lambda: 0

evaluate:
  mode: "final_onehot" # mode of CLIP cache to evaluate
  batch_size: 8
  num_workers: 4

train:
  seed: 42
  target_transform: filename_final_onehot # 'filename_<pseudo label mode: [global, aggregate, final]>_onehot'
  train_transform:
    _target_: src.data.data.ResNet101Transforms
  val_transform:
    _target_: src.data.data.ResNet101Transforms
  batch_size: 8
  num_workers: 16
  device: cuda
  optimizer:
    _target_: torch.optim.Adam
    lr: 1e-5
  loss:
    _target_: torchmetrics.regression.KLDivergence
  # warmup epochs before alternate updates
  warmup: 0
  # frequency (in epochs) while alternating updates of psuedo labels
  pseudo_update_frequency: 10
  sigma: 1
  enable_checkpointing: false
  resume:
    ckpt_path: null
  max_epochs: 100
  # stop the run if mAP of pseudo labels is less than initial. Use this for finding hparams
  early_stopping: 3 # check from initial update