# @package _global_

# specify default configuration
# order of defaults determines the order in which configs override each other
defaults:
  - _self_
  - logger: wandb
  - paths: voc2012
  - data: voc2012
  - hydra: default
  - model: default

  # experiment configs allow for version control of specific hyperparameters
  # e.g. best hyperparameters for given model and datamodule
  - experiment: null

  # config for hyperparameter optimization
  # - hparams_search: null

# task name, determines output directory path (for logs)
task_name: "clip_cache"

# config for creating CLIP cache vectors
clip_cache:
  batch_size: 8
  thresh: 0.5
  snippet_size: 3
  global_cache_dir: ${paths.cache_save_root}/clip_cache/model_${model.clip.name}/global
  aggregate_cache_dir: ${paths.cache_save_root}/clip_cache/model_${model.clip.name}/thresh_${clip_cache.thresh}/snippet_size_${clip_cache.snippet_size}/aggregate

evaluate: 
  mode: "global_onehot" # mode of CLIP cache to evaluate
  batch_size: 8
  num_workers: 4

train:
  seed: 42
  model:
    _target_: torchvision.models.resnet101
    weights: DEFAULT
  target_transform: global_onehot # '<pseudo label mode>_onehot'
  batch_size: 8
  num_workers: 4
  device: cuda
  optimizer:
    _target_: torch.optim.Adam
    lr: 1e-5
  loss: 
    _target_: torch.nn.KLDivLoss
    reduction: batchmean # see https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html#kldivloss
  enable_checkpointing: false
  resume:
    ckpt_path: null
  max_epochs: 20

